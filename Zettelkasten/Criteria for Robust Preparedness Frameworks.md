
Created on: 04-03-2025 15:16
Status: #idea
Tags: #ai_gov #AI_eval #ai_safety 
# Criteria for Robust Preparedness Frameworks
## Breadth
> What risks are covered?
![[Breadth of Preparedness Frameworks#Breadth of Potential Catastrophic Risks]]

## Risk Appetite
>What is an acceptable level of risk?
![[Risk Appetite for Preparedness Frameworks#Risk Appetite for Preparedness Frameworks]]

## Clarity
> Clearly define capability levels and risk thresholds, and should be robust enough to hold developers accountable.
> ![[Clarity of Preparedness Frameworks#Under-specified qualitative thresholds]]

## Evaluation
>What tests will developers run on AI models.
>![[AI Evaluation (for Preparedness Frameworks)#AI Evaluation (for Preparedness Frameworks)]]

## Mitigations
>What will be done when they cross the risk threshold?
>![[Mitigations for Preparedness Frameworks#Risk Mitigations]]

## Robustness
>Ensure that risk mitigations work.
>![[Robustness of Preparedness Frameworks#Robustness of Preparedness Frameworks]]

## Accountability
>How to ensure that AI developers are held accountable to their commitments to safety. 
>![[Accountability of PFs#Accountability of PFs]]

## Amendments
> How will developers change their PFs over time?
> ![[Amendments to PFs#Amendments to PFs]]

## Transparency
>AI developers should communicate their capabilities and risk research.
>![[Transparency of Evals in PFs#Transparency of Evals in PFs]]


-----------------


# References
[Can Preparedness Frameworks Pull Their Weight?](https://fas.org/publication/scaling-ai-safety/)