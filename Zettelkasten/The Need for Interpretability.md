Created on: 30-06-2025 01:08, note by Youssef Okeil
Status: #idea
Tags: #AI/Safety #AI/Safety/Interpretability
# The Need for Interpretability
before deploying the system, any method of evaluating the system can only be a proxy for its actual performance. The most common way is by testing the system performance on a test set or environment. 

**This approach fails to reveal & incentivize:**
- deception
- biases
- over-fitting

That's why we need an interpretability toolbox.

### For [[Deceptive Alignment]]:
 while it's not uniquely useful for deceptive alignment, it seems well-equipped for detecting ways of deceptive alignment.


-----------------
# References
https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7/p/ExRN5Bu3696cf9Ccm