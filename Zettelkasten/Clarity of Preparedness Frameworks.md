Created on: 04-03-2025 15:40
Status: #idea
Tags: #AI/Safety #AI/Governance #AI/evals 
### Clarity of Preparedness Frameworks
This has a bit of an overlap with risk appetite as the acceptable level of risk should be clearly defined, to leave little room for ambiguity to be used by the developers

#### OpenAI's High risk threshold
- Model enables an expert to develop a novel threat
- ... or improved assistance that enables anyone with basic training in a relevant field to create a CBRN threat
#### Anthropic's ASL-3 (AI Safety Level)
- Low-level autonomous capabilities or access to the model would substantially increase the risk of catastrophic misuse. By lowering costs or enabling new methods of attack.
#### Under-specified qualitative thresholds:
- "meaningfully improved assistance", "substantial increase in risk of catastrophic misuse" those should be quantified as much as possible
- We lack good empirical understanding of quantification of AI-risks. This is a new science.



-----------------
# References