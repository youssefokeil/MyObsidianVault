---
kindle-sync:
  bookId: '55134'
  title: AGI safety from first principles
  highlightsCount: 59
---
# AGI safety from first principles
## Metadata


## Highlights
The key concern motivating technical AGI safety research is that we might build autonomous artifcially intelligent agents which are much more intelligent than humans, and which pursue goals that confict with our own — location: []() ^ref-43525

---
If they don’t want to obey us, then humanity might become only Earth’s second most powerful \species", and lose the ability to create a valuable and worthwhile future — location: []() ^ref-63900

---
I also frequently compare AI development to the evolution of human intelligence; while the two aren’t fully analogous, humans are the best example we currently have to ground our thinking about generally intelligent AIs — location: []() ^ref-5923

---
superintelligence, we should frst characterise what we mean by intelligence. We can start with Legg and Hutter [2007]’s well-known — location: []() ^ref-13159

---
Superintelligence In order to understand superintelligence, we should frst characterise what we mean by intelligence. We — location: []() ^ref-46508

---
versus agents which can understand new tasks with little or no task-specifc training, by generalising from previous experience (the generalisation-based approach). — location: []() ^ref-3676

---
- but even though they can process arbitrarily many diferent inputs, detailed instructions for how to do that processing needs to be individually written to build each piece of software — location: []() ^ref-6727

---
Starcraft, DOTA, Go, and so on — location: []() ^ref-20328

---
An example of the generalisation-based approach can be found in large language models like GPT-2 and GPT-3. GPT-2 was frst trained on the task of predicting the next word in a corpus, and then achieved state of the art results on many other language tasks, without any task-specifc fne-tuning! [Radford et al., 2019 — location: []() ^ref-16515

---
Its successor, GPT-3, has displayed a range of even more impressive behaviour [Sotala, 2020]. I think this provides a good example of how an AI could develop cognitive skills (in this case, an understanding of the syntax and semantics of language — location: []() ^ref-7899

---
As a species, we were \trained" by evolution to have cognitive skills including rapid learning capabilities; sensory and motor processing; and social skills — location: []() ^ref-45389

---
As individuals, we were also \trained" during our childhoods to fne-tune those skills; to understand spoken and written language; and to possess detailed knowledge about modern society. — location: []() ^ref-47635

---
In particular, the skill of abstraction allows us to extract common structure from diferent situations, which allows us to understand them much more efciently than by learning about them one by one. — location: []() ^ref-37934

---
By contrast, there are many economically important tasks which I expect AI systems to do well at primarily by generalising from their experience with very diferent tasks - meaning that those AIs will need to generalise much, much better than our current reinforcement learning systems can. — location: []() ^ref-38664

---
based approach will get a long way in areas where we can gather a lot of data. For example, I’m confdent that it will produce superhuman self-driving cars well — location: []() ^ref-33118

---
For example, I’m confdent that it will produce superhuman self-driving cars well before the generalisation-based approach does so. It may also allow us to automate most of the tasks involved even in very cognitively — location: []() ^ref-49618

---
However, some jobs crucially depend on the ability to analyse and act on such a wide range of information that it’ll be very difcult to train directly for high performance on them — location: []() ^ref-40848

---
But I expect that well before such an efort becomes possible, we’ll have built AIs using the generalisation-based approach which know how to perform well even — location: []() ^ref-1860

---
One potential obstacle to the generalisation-based approach succeeding is the possibility that specifc features of the ancestral environment, or of human brains, were necessary for general intelligence to arise [Ngo, 2020b — location: []() ^ref-31089

---
which may be very diferent from the tasks we actually want an AI CEO to do) — location: []() ^ref-34545

---
For example, we could train a reinforcement learning agent to follow instructions in a simulated world — location: []() ^ref-44154

---
Let’s call these systems artifcial general intelligences, or AGIs. Many AI researchers expect that we’ll build AGI within this century [Grace et al., 2018]; however, I won’t explore arguments around the timing of AGI development, and the rest of this document doesn’t depend on this question — location: []() ^ref-33071

---
humanity could if we coordinated globally (unaided by other advanced AI). I think it’s difcult to deny that in principle it’s possible to build individual generalisation-based AGIs which are superintelligent, since human brains are constrained by many factors4 which will be much less limiting for AIs. — location: []() ^ref-36099

---
I think it’s difcult to deny that in principle it’s possible to build individual generalisation-based AGIs which are superintelligent, since human brains are constrained by many factors4 which will be much less limiting for AIs. Perhaps the most striking is the vast diference between the speeds of neurons and transistors: the — location: []() ^ref-30254

---
the latter pass signals about four million times more quickly — location: []() ^ref-27982

---
And while evolution is a very capable designer in many ways, it hasn’t had much time to select specifcally for the skills that are most useful in our modern environment, such as linguistic competence and mathematical reasoning — location: []() ^ref-17092

---
By default, we should expect that it will be driven by the standard factors which infuence progress in AI: more compute, better algorithms, and better training data — location: []() ^ref-63095

---
Duplication currently allows us to apply a single AI to many tasks, but not to expand the range of tasks which that AI can achieve — location: []() ^ref-17322

---
which can carry out signifcantly more complex tasks than the original can.6 Because of the ease and usefulness of duplicating an AGI, I think that collective AGIs should be our default expectation for how superintelligence will be deployed — location: []() ^ref-58577

---
However, most of the arguments given in the previous paragraphs are also reasons why individual AGIs will be able to surpass us at the skills required — location: []() ^ref-24585

---
One particularly useful skill is cultural learning: we should expect AGIs to be able to acquire knowledge from each other and then share their own discoveries in turn, allowing a collective AGI to solve harder problems than any individual AGI within it could — location: []() ^ref-14064

---
Thirdly, AGIs will be able to improve the training processes used to develop their successors, which then improve the training processes used to develop their successors, and so on, in a process of recursive improvement — location: []() ^ref-7405

---
So it’s probably more accurate to think about self-modifcation as the process of an AGI modifying its high-level architecture or training regime, then putting itself through signifcantly more training — location: []() ^ref-16116

---
And for an AGI to trust that its goals will remain the same under retraining will likely require it to solve many of the same problems that the feld of AGI safety is currently tackling — location: []() ^ref-50642

---
Indeed, since AIs will eventually be the primary contributors to AI research, recursive improvement as defned here will eventually become the key driver of progress. — location: []() ^ref-30952

---
AIs pursue power for the sake of achieving other goals; i.e. power is an instrumental goal for them — location: []() ^ref-43572

---
Why might they end up with that power — location: []() ^ref-6350

---
AIs pursue power for its own sake; i.e. power is a fnal goal for them. 3. AIs gain power without aiming towards it; e.g. because humans gave it to them. — location: []() ^ref-41090

---
instrumental convergence thesis, which states that there are some instrumental goals whose attainment would increase the chances of an agent’s fnal goals being realised for a wide range of fnal goals and a wide range of situations — location: []() ^ref-27720

---
Examples of such instrumentally convergent goals include self-preservation, resource acquisition, technological development, and self-improvement, which are all useful for executing further large-scale plans. — location: []() ^ref-20838

---
I’ll cover it briefy in this section and the next. Following Christiano [2019], I’ll call agents which fall into either of these frst two categories infuence-seeking — location: []() ^ref-4486

---
It’s not yet clear that AGIs will be this type of agent, or have this type of goals — location: []() ^ref-33660

---
for example by earning and saving money, and can imagine how much better we’d be at them if we were more intelligent — location: []() ^ref-50369

---
goals. Our conquest of the world didn’t require any humans to strategise over the — location: []() ^ref-61188

---
many useful short-term drives (in particular the drive towards power itself), it’s difcult to determine the extent to which human infuence-seeking behaviour is caused by us reasoning about its instrumental usefulness towards larger-scale goals — location: []() ^ref-4205

---
We can imagine them possessing fnal goals which don’t incentivise the pursuit of power, such as deontological goals, or small-scale goals. Or — location: []() ^ref-13792

---
begin, it’s crucial to distinguish between the goals which an agent has been selected or designed to do well at (which I’ll call its design objectives9), and the goals which an agent itself wants to achieve (which I’ll just call \the agent’s goals") — location: []() ^ref-8469

---
they can be competent at achieving their design objectives without understanding what those objectives are, or how their actions will help achieve them — location: []() ^ref-21460

---
design objective is to accumulate power, but — location: []() ^ref-40076

---
Three existing frameworks which attempt to answer this question are Morgenstern and Von Neumann [1953]’s expected utility maximisation — location: []() ^ref-1954

---
Dennett [1989]’s intentional stance, and Hubinger et al. [2019]’s mesa-optimisation — location: []() ^ref-38082

---
While we can prove elegant theoretical results about utility functions, they are such a broad formalism that practically any behaviour can be described as maximising some utility function [Ngo, 2019b] — location: []() ^ref-45393

---
expectations about powerful AGIs.11 Meanwhile — location: []() ^ref-53541

---
Predicting the behaviour of a trillion-parameter neural network is very diferent from applying the intentional stance to existing artifacts — location: []() ^ref-33860

---
a system is an optimizer if it is internally searching through a search space (consisting of possible outputs, policies, plans, strategies, or similar) looking for those elements that score high according to some objective function that is explicitly represented within the system" — location: []() ^ref-58679

---
instance, it highlights that agency requires a combination of diferent abilities - and as a corollary, that there are many diferent ways to be less than maximally agentic — location: []() ^ref-23975

---
, intelligence seems intrinsically linked to a frst-person perspective. But an AGI trained on abstract third-person data might develop a highly sophisticated world-model that just doesn’t include itself or its outputs — location: []() ^ref-65101

---
sufciently advanced language or physics model might ft into this category. — location: []() ^ref-3842

---
for instance, an agent is only trained to consider restricted types of plans. Myopic training attempts to implement such agents; more generally, an agent could have limits on the actions it considers — location: []() ^ref-59422

---
